@misc{abadiTensorFlowLargescaleMachine2015,
  title = {{{TensorFlow}}: {{Large-scale}} Machine Learning on Heterogeneous Systems},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015},
  url = {https://www.tensorflow.org/},
  keywords = {ad,tracer}
}

@article{baydinAutomaticDifferentiationMachine2018,
  title = {Automatic {{Differentiation}} in {{Machine Learning}}: A {{Survey}}},
  shorttitle = {Automatic {{Differentiation}} in {{Machine Learning}}},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {153},
  pages = {1--43},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v18/17-468.html},
  urldate = {2023-03-12},
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply {\^a}autodiff{\^a}, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names {\^a}dynamic computational graphs{\^a} and {\^a}differentiable programming{\^a}. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms {\^a}autodiff{\^a}, {\^a}automatic differentiation{\^a}, and {\^a}symbolic differentiation{\^a} as these are encountered more and more in machine learning settings.},
  keywords = {ad,autodiff,done,inferopt,thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/MP87JETA/Baydin et al_2018_Automatic Differentiation in Machine Learning.pdf}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {1},
  pages = {65--98},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/141000671},
  url = {https://epubs.siam.org/doi/10.1137/141000671},
  urldate = {2022-12-03},
  langid = {english},
  keywords = {ad,bootstrap,gnn,hmm,inferopt,povar,roadef25,sparse_ad_blog,thesis,tracer,viva},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/YWLISSFK/Bezanson et al_2017_Julia.pdf}
}

@inproceedings{blondelEfficientModularImplicit2022,
  title = {Efficient and {{Modular Implicit Differentiation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and {Llinares-L{\'o}pez}, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
  year = {2022},
  month = oct,
  url = {https://openreview.net/forum?id=Q-HOv_zn6G},
  urldate = {2023-03-12},
  abstract = {Automatic differentiation (autodiff) has revolutionized machine learning. It allows to express complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization layers, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, so far, implicit differentiation remained difficult to use for practitioners, as it often required case-by-case tedious mathematical derivations and implementations. In this paper, we propose automatic implicit differentiation, an efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function \$F\$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of \$F\$ and the implicit function theorem to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many existing implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.},
  langid = {english},
  keywords = {ad,autodiff,done,inferopt,thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/5MNWEQPU/Blondel et al. - 2022 - Efficient and Modular Implicit Differentiation.pdf}
}

@misc{blondelElementsDifferentiableProgramming2024,
  title = {The {{Elements}} of {{Differentiable Programming}}},
  author = {Blondel, Mathieu and Roulet, Vincent},
  year = {2024},
  month = jul,
  number = {arXiv:2403.14606},
  eprint = {2403.14606},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.14606},
  url = {http://arxiv.org/abs/2403.14606},
  urldate = {2024-08-06},
  abstract = {Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.},
  archiveprefix = {arXiv},
  keywords = {ad,autodiff,done,roadef25,sparse_ad_blog,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/6LFEKUZA/Blondel_Roulet_2024_The Elements of Differentiable Programming.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/7VQWVTKF/2403.html}
}

@inproceedings{bolteMathematicalModelAutomatic2020,
  title = {A Mathematical Model for Automatic Differentiation in Machine Learning},
  booktitle = {Conference on {{Neural Information Processing Systems}}},
  author = {Bolte, Jerome and Pauwels, Edouard},
  year = {2020},
  month = dec,
  address = {Vancouver, Canada},
  url = {https://hal.archives-ouvertes.fr/hal-02734446},
  urldate = {2021-08-18},
  abstract = {Automatic differentiation, as implemented today, does not have a simple mathematical model adapted to the needs of modern machine learning. In this work we articulate the relationships between differentiation of programs as implemented in practice and differentiation of nonsmooth functions. To this end we provide a simple class of functions, a nonsmooth calculus, and show how they apply to stochastic approximation methods. We also evidence the issue of artificial critical points created by algorithmic differentiation and show how usual methods avoid these points with probability one.},
  keywords = {ad,todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/VTGLTLYS/Bolte_Pauwels_2020_A mathematical model for automatic differentiation in machine learning.pdf}
}

@misc{bradburyJAXComposableTransformations2018,
  title = {{{JAX}}: Composable Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and {Wanderman-Milne}, Skye and Zhang, Qiao},
  year = {2018},
  url = {http://github.com/google/jax},
  keywords = {ad,sparse_ad_blog,tracer}
}

@book{griewankEvaluatingDerivativesPrinciples2008,
  title = {Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation},
  shorttitle = {Evaluating Derivatives},
  author = {Griewank, Andreas and Walther, Andrea},
  year = {2008},
  edition = {2nd ed},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, PA},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9780898717761},
  abstract = {This title is a comprehensive treatment of algorithmic, or automatic, differentiation. The second edition covers recent developments in applications and theory, including an elegant NP completeness argument and an introduction to scarcity},
  isbn = {978-0-89871-659-7},
  lccn = {QA304 .G76 2008},
  keywords = {ad,autodiff,inferopt,roadef25,semidone,sparse_ad_blog,thesis,tracer},
  annotation = {OCLC: ocn227574816},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/ZBNZIFGC/Griewank_Walther_2008_Evaluating Derivatives.pdf}
}

@misc{huckelheimUnderstandingAutomaticDifferentiation2023,
  title = {Understanding {{Automatic Differentiation Pitfalls}}},
  author = {H{\"u}ckelheim, Jan and Menon, Harshitha and Moses, William and Christianson, Bruce and Hovland, Paul and Hasco{\"e}t, Laurent},
  year = {2023},
  month = may,
  number = {arXiv:2305.07546},
  eprint = {2305.07546},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07546},
  url = {http://arxiv.org/abs/2305.07546},
  urldate = {2024-11-05},
  abstract = {Automatic differentiation, also known as backpropagation, AD, autodiff, or algorithmic differentiation, is a popular technique for computing derivatives of computer programs accurately and efficiently. Sometimes, however, the derivatives computed by AD could be interpreted as incorrect. These pitfalls occur systematically across tools and approaches. In this paper we broadly categorize problematic usages of AD and illustrate each category with examples such as chaos, time-averaged oscillations, discretizations, fixed-point loops, lookup tables, and linear solvers. We also review debugging techniques and their effectiveness in these situations. With this article we hope to help readers avoid unexpected behavior, detect problems more easily when they occur, and have more realistic expectations from AD tools.},
  archiveprefix = {arXiv},
  keywords = {ad,todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/K4DFBVM7/Hückelheim et al. - 2023 - Understanding Automatic Differentiation Pitfalls.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/BQBZAZVG/2305.html}
}

@article{mandiDecisionFocusedLearningFoundations2024,
  title = {Decision-{{Focused Learning}}: {{Foundations}}, {{State}} of the {{Art}}, {{Benchmark}} and {{Future Opportunities}}},
  shorttitle = {Decision-{{Focused Learning}}},
  author = {Mandi, Jayanta and Kotary, James and Berden, Senne and Mulamba, Maxime and Bucarey, Victor and Guns, Tias and Fioretto, Ferdinando},
  year = {2024},
  month = aug,
  journal = {Journal of Artificial Intelligence Research},
  volume = {80},
  pages = {1623--1701},
  issn = {1076-9757},
  doi = {10.1613/jair.1.15320},
  url = {https://www.jair.org/index.php/jair/article/view/15320},
  urldate = {2024-11-27},
  abstract = {Decision-focused learning (DFL) is an emerging paradigm that integrates machine learning (ML) and constrained optimization to enhance decision quality by training ML models in an end-to-end system. This approach shows significant potential to revolutionize combinatorial decision-making in real-world applications that operate under uncertainty, where estimating unknown parameters within decision models is a major challenge. This paper presents a comprehensive review of DFL, providing an in-depth analysis of both gradient-based and gradient-free techniques used to combine ML and constrained optimization. It evaluates the strengths and limitations of these techniques and includes an extensive empirical evaluation of eleven methods across seven problems. The survey also offers insights into recent advancements and future research directions in DFL.},
  copyright = {Copyright (c) 2024 Journal of Artificial Intelligence Research},
  langid = {english},
  keywords = {ad,roadef25},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/M5F88946/Mandi et al. - 2024 - Decision-Focused Learning Foundations, State of the Art, Benchmark and Future Opportunities.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/XSXDGJ6F/Mandi et al. - 2023 - Decision-Focused Learning Foundations, State of t.pdf}
}

@article{margossianReviewAutomaticDifferentiation2019,
  title = {A Review of Automatic Differentiation and Its Efficient Implementation},
  author = {Margossian, Charles C.},
  year = {2019},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {e1305},
  issn = {1942-4795},
  doi = {10.1002/widm.1305},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1305},
  urldate = {2023-03-12},
  abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation (AD) is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of AD, however, requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region-based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open and practical problems include the extension of current packages to provide more specialized routines, and finding optimal methods to perform higher-order differentiation. This article is categorized under: Algorithmic Development {$>$} Scalable Statistical Methods},
  langid = {english},
  keywords = {ad,autodiff,done,inferopt,thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/G6C6EX8U/Margossian_2019_A review of automatic differentiation and its efficient implementation.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/2JWFI24S/widm.html}
}

@article{mohamedMonteCarloGradient2020,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {132},
  pages = {1--62},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v21/19-346.html},
  urldate = {2022-10-21},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies---the pathwise, score function, and measure-valued gradient estimators---exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  keywords = {ad,autodiff,diffexp,done},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/6KTY5IG4/Mohamed et al. - 2020 - Monte Carlo Gradient Estimation in Machine Learnin.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/IMI4JXES/mc_gradients.html}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  urldate = {2024-05-06},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  keywords = {ad,No DOI found,sparse_ad_blog,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/Z6BMB8KI/Paszke et al_2019_PyTorch.pdf}
}

@misc{sapienzaDifferentiableProgrammingDifferential2024,
  title = {Differentiable {{Programming}} for {{Differential Equations}}: {{A Review}}},
  shorttitle = {Differentiable {{Programming}} for {{Differential Equations}}},
  author = {Sapienza, Facundo and Bolibar, Jordi and Sch{\"a}fer, Frank and Groenke, Brian and Pal, Avik and Boussange, Victor and Heimbach, Patrick and Hooker, Giles and P{\'e}rez, Fernando and Persson, Per-Olof and Rackauckas, Christopher},
  year = {2024},
  month = jun,
  number = {arXiv:2406.09699},
  eprint = {2406.09699},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.09699},
  url = {http://arxiv.org/abs/2406.09699},
  urldate = {2024-11-22},
  abstract = {The differentiable programming paradigm is a cornerstone of modern scientific computing. It refers to numerical methods for computing the gradient of a numerical model's output. Many scientific models are based on differential equations, where differentiable programming plays a crucial role in calculating model sensitivities, inverting model parameters, and training hybrid models that combine differential equations with data-driven approaches. Furthermore, recognizing the strong synergies between inverse methods and machine learning offers the opportunity to establish a coherent framework applicable to both fields. Differentiating functions based on the numerical solution of differential equations is non-trivial. Numerous methods based on a wide variety of paradigms have been proposed in the literature, each with pros and cons specific to the type of problem investigated. Here, we provide a comprehensive review of existing techniques to compute derivatives of numerical solutions of differential equations. We first discuss the importance of gradients of solutions of differential equations in a variety of scientific domains. Second, we lay out the mathematical foundations of the various approaches and compare them with each other. Third, we cover the computational considerations and explore the solutions available in modern scientific software. Last but not least, we provide best-practices and recommendations for practitioners. We hope that this work accelerates the fusion of scientific models and data, and fosters a modern approach to scientific modelling.},
  archiveprefix = {arXiv},
  keywords = {ad,roadef25,sparse_ad_blog,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/IRWQE3LN/Sapienza et al. - 2024 - Differentiable Programming for Differential Equations A Review.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/WKFD9DSU/2406.html}
}
